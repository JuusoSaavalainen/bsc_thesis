\chapter{Multimodal machine learning with biomedical data\label{Essay}}



Machine learning methods have increasingly more potential in the field of medicine. Possible applications are for example early diagnosis, risk prediction, treatment planning, and prognosis estimation. Biomedical data can be seen as data that holds information about a person's health status. Already existing clinical data cohorts, electronic health records, and data registries hold a wide range of heterogeneous multimodal biomedical data that could be utilized with machine learning to potentially enhance various aspects of health care.  However, heterogeneity, modality, and sensitivity bring many challenges to developing useful machine-learning applications with such data. 

Precision health studies the usage of biomedical data to produce individual-level custom healthcare. Successfully understanding the genomics and other health data associated with certain diseases could ultimately lead to better treatment and prevention. Electronic health records (EHR) can hold a wide variety of data and can potentially be enhanced from different sources such as registries and biobanks. EHR can also be used to mean only the tabular data. However, EHRs can consist of multiple modalities such as tabular, image, time-series, structured sequence, and unstructured sequence and here we refer to EHRs as an umbrella term for an entity holding all the biomedical data from multiple modalities. The complexity, high variety in formats, and missing values are some of the typical issues when such data is used. Retrospective studies look at the history and gather data from the past, often relying on EHR/registry data and producing longitudinal datasets to study a certain disease. Especially the usage of historical registries exposes these studies to information bias. Missing values and nonfixed size on the longitudinal measurements also pose issues with effective usage. Prospective studies on the other are studies that gather data from the current state and are seen as more stable in terms of bias but both come with an issue regarding the generalisability. Evaluation of the data sources and their correctness is essential when dealing with heterogeneous data and multiple modalities. 

While the usage of machine learning in the context of medicine is not only limited to predicting tasks, it is one of the most interesting applications. Diagnosing certain diseases that may be hard to diagnose or it's otherwise crucial to diagnose them early is one of the tasks that machine learning could be used for. The decision-making on diagnosis could be bound to just one lab value but often is tied to information coming from multiple modalities such as images and longitudinal clinical measurements. Single-modal machine learning models have successfully recognized different outliers in medical imaging using convolutional neural networks. In practice the medical images are used combined with information from other modalities, so how could we use multiple modalities of relevant data within the machine learning architectures? Multimodal machine learning aims to use multiple modalities of the data together to derive more accurate predictions. Using multimodal models increases the complexity compared to single-modal models and comes with many challenges and limitations. 

Data fusion takes on the integration of the modalities of the data to multimodal machine learning models. The data fusion techniques can be divided into three main categories: early, intermediate (joint), and late fusion, but other architectures exist as well.  In early fusion, the data is joined into a single feature vector before it is fed into the model. This can be done for example with pooling or concatenation.
Joint or intermediate fusion uses the fusing in the intermediate layers. With neural networks at the bottom, first, the feature representation is learned and then joined with other modalities to be fed as input to the model that produces the final output. Joint fusion can be separated into type 1 and type 2. Type 1 uses feature extraction in the beginning for all modalities. Type 2 is referred to when the feature extraction layer is not used for all modalities as some features can be used without this step. With joint fusion and the neural networks used in this context, we are able to use the backpropagation from the output to the bottom parts and produce also better feature representations. 
Late fusion uses single modalities each with its own model and uses fusion with the outputs of those models with aggregation before the final output. All of these methods have shown potential compared to models using a single modality for the same task. 
It is worth noting that increasing the number of modalities does not always yield better results compared to single-modal models but has shown better results in many cases.

One recent study proposed a framework for generating and testing multimodal models called Holistic AI in Medicine (HAIM). The proposed framework offers a modular pipeline that allows easy adaptation of different modalities and classification tasks. The framework was used with 4 different modalities (tabular, time series, text, image) and 11 data sources. From each modality, embeddings are generated individually to then be concatenated into fusion embedding that can be used on predicting tasks. For these embeddings pre-trained neural networks and statistical methods were used to extract the features.  HAIM also offers a possibility to add additional modalities and update the feature extractors without a need to train other extractors again. This modularity allows researchers or developers to use state of the art models easily and adapt the framework to their needs.
Validation for this proposed framework was done by comparison of the different statistical metrics such as the average area under the receiver operating characteristic on models using multiple modalities and sources to single-modal models. The prediction tasks that they used in this comparison were pathology diagnosis, 48h mortality, and length of stay. Gradient boosted trees (XGBoost) were used in all of these tasks and 14 324 models trained with almost all possible combinations of modalities and sources. Some models that used unfair combinations of data were excluded. Multimodal models improved performance across all tasks showing promises on the multimodal approach. HAIM shows the potential of using multiple modalities and also proposes an open source framework with modular architecture that could improve the research and development of multimodal machine learning models in the future. 


1. Cascarano, A. et al. Machine and deep learning for longitudinal biomedical data: a review of methods and applications. Artif Intell Rev 56, 1711–1771 (2023).

2. Acosta, J. N., Falcone, G. J., Rajpurkar, P. & Topol, E. J. Multimodal biomedical AI. Nat Med 28, 1773–1784 (2022).

3. Kline, A. et al. Multimodal machine learning in precision health: A scoping review. npj Digit. Med. 5, 1–14 (2022).

4. Shih-Cheng, H. et al. Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. NPJ Digital Medicine 3, (2020).

5. Soenksen, L. R. et al. Integrated multimodal artificial intelligence framework for healthcare applications. npj Digit. Med. 5, 1–10 (2022).

https://www.overleaf.com/project/65a9032765bd7073d0e97835



# notes
ensemble vs late fusion selkeytys
