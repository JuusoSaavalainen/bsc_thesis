\chapter{Applications of multimodal machine learning\label{discussion}}

A recent literature review and highlighted the increased interest in multimodal machine learning (MML) research (\cite{litrewMMLapplic}). In theory, MML approaches can be applied to any domain or problem that links to multiple modalities already solved with machine learning. While the concept of data fusion is not new, deep learning has enabled many possibilities in this field. Particularly within the medical context, data fusion, and MML are a relatively new and currently researched area. Applications of MML appear in recent research papers, but not yet in clinical environments.

\section{Applications in medicine}

\begin{table}[]
\begin{tabular}{lllll}
\hline
                         & Classification task  & Modalities & Fusion approach         &  \\ \hline
Cahan et al., 2023       & PE risk  & 2          & early,late,intermediate &  \\
Venugopalan et al., 2021 & AD, MCI, controls  & 3          & early,late,intermediate                   &  \\
Soenksen et al., 2022    & Multiple & 2-4 (2-11) & early                   &  \\ \hline
\end{tabular}
\end{table}

The medical domain provides many problems that could benefit from MML approach. Biobanks, health centers, registers, and research provide large amounts of different sets of modalities. Literature review ranging from years 2011–2021 of MML in health for diagnosis/prognosis tasks found most papers were from neurology and oncology domains (\cite{articlePreciRev}). Recent Finish article: "The mathematics and statistical models in predicting treatment response in cancer" pointed out, that one of the biggest challenges is the efficient integration of different modalities to achieve the best predictive accuracy (\cite{duodecim}). Biomedical data opens opportunities for a wide amount of other tasks than prognosis and diagnosing. Potential use cases could be in drug discovery, remote monitoring, and many other applications in healthcare (\cite{acosta2022multimodal}). This section focuses on the studies on prognosis and diagnosis as downstream tasks and showcases the use of 2, 3, and 3+ modalities in their respective order. All of these showcased studies are retrospective. It's important to note that usage of machine learning in medical context does not necessarily aim to remove the work of physicians but may help them for example to identify those patients that need specific treatment immediately. Similarly, MML can help to identify the possibility of some diseases earlier. 

\cite{pulmembol} applied MML methods to predict the risk stratification of pulmonary embolism (PE). Evaluation and comparison were done by building multiple models with different fusion strategies in this retrospective study. Three encoders were built to be used as encoders in multimodal settings and as unimodal baseline models for image and EHR modalities. Previously, \cite{pulm1st} introduced CNN-based Stacked Attention Network (SANet), which was used as the main imaging encoder, but also compared to the second baseline image encoder, Swin UNETR Transformer (\cite{SWIN}), in intermediate fusion architectures. Futhermore, TabNet (\cite{arik2020tabnet}), a neural network architecture for tabular data was used as a baseline encoder for EHR modality. For downstream tasks, XGBoost or TabNet was used. Additionally, for early and intermediate fusion architectures, dimensionality reduction methods were evaluated for the fusion layer after concatenation. The bilinear attention network (BAN) (\cite{kim2018bilinear}) was chosen as an additional layer to further enhance the fusion. The best-performing architecture was an intermediate fusion with SANet and TabNet encoders followed by concatenation and BAN for the fusion layer before the last TabNet as a downstream classifier. This architecture significantly increased AUC over the unimodal baseline models showing the possible value gained by using multimodal architecture. Identifying PE early and proper treatment can decrease the deaths caused by this disease (\cite{pulmembol}).


\cite{alzmulti} presented MML approach to classify Alzheimer’s disease (AD), mild cognitive disorders (MCI), and controls. Imaging (MRI), genetic (SNP), and clinical (EHR) modalities were used and evaluated in different combinations. The Alzheimer’s Disease Neuroimaging Initiative (ADNI) (\cite{ADNI}) was used as a data source for this study. The backbone of the Deep MML architecture was built using feature extraction for each modality. MRI features extracted with 3D CNN. SNP and EHR data features were similarly extracted with stacked denoising auto-encoders. Each of these models was trained independently. Extracted features are then used as input after concatenation for the classifier layer making it an early fusion approach. Other deep fusion approaches were not evaluated. K-nearest neighbors (kNN), support vector machines (SVM), random forests, and decision trees were compared as downstream classifiers. This model was compared to shallow models with early and late fusion approaches. Both shallow models employed decision trees and the late version used majority voting. A comparison between models using single modalities was done to highlight the performance of deep models over shallow ones. Deep unimodal approaches outperformed kNN, decision trees, random forests, and SVM approaches with SNP and MRI modalities. Decision trees and random forests achieved similar performance with deep approaches in EHR modality. Similarly, the two shallow models were compared to deep models with kNN, SVM, decision trees, and random forests as downstream classifiers in all possible combinations of modalities. All deep approaches performed better compared to shallow models in every combination of modalities excluding the MRI + SNP. This was explained with small sample sizes. This study demonstrated the possible benefits that can be gained with deep fusion approaches. Similar findings were found in review which stated that leveraging deep learning fusion consistently showed improvements in performance for Alzheimer's disease diagnosis (\cite{article22}). Identifying the AD or MCI early would be beneficial for patients and society as the costs decrease and the progression slowed (\cite{rasmussen2019alzheimer}).


\cite{articlehaim} introduced and evaluated a unified holistic AI in medicine (HAIM) framework. HAIM is built to handle nonfixed-size sets of modalities. Each modality is handled independently with feature extraction models that generate a set of embeddings that can then be used for multiple downstream tasks. This architecture can be modified with any set of modalities and feature extraction models to whatever state-of-the-art models are available. This is seen as early fusion architecture where the machine learning models act as feature extractors. The evaluation of the proposed framework was done with open-source data from MIMIC-IV (\cite{articlemimic4}) and MIMIC-CXR-JPG (\cite{mimicXR}). Images, tabular data, natural language, and time series were used as modalities with 11 sources. For images pre-trained Densenet121 (\cite{densenet}) was used as a feature extractor, similarly for natural language pre-trained Clinical BERT (\cite{bertclinic}) was used. The tasks for evaluation included 10 different chest diagnoses, length-of-stay, and 48h-mortality. XGBoost was used as the final classifier for all of these downstream tasks. Shapley values were reported to analyze the benefits gained from different modalities and models with different combinations of data sources and modalities compared to single-source models. Vision data was found to contribute most to performance in chest diagnostic tasks, while length-of-stay and 48-hour mortality time-series were identified as the most relevant modalities. Multimodal approaches consistently outperformed the single-source models in AUROC with an average improvement of 9-28\% across all downstream tasks. 


\section{Applications in other fields}

Multimodal machine learning can be found in a wide variety of applications and research from multimedia, robotics, and human-computer interaction (\cite{liang2023foundations}). While each field approaches problems from a different angle, the solution can benefit many. Autonomous vehicles could benefit from using multiple modalities but must also handle the data quickly in real-time. \cite{prakash2021multi} introduced a model called TransFuser that utilizes multiple fusion layers between convolutional layers in feature extractors for LiDAR and front-facing cameras. Interestingly the latest versions of widely popular large language models are introduced as multimodal models as they can out and input multiple modalities (\cite{openai2024gpt4} \cite{geminiteam2023gemini}). 

